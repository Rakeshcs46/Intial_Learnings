{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36242079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "infersent = torch.load('InferSent/encoder/infersent.allnli.pickle', map_location=lambda storage, loc: storage)\n",
    "infersent.set_glove_path(\"InferSent/dataset/GloVe/glove.840B.300d.txt\")\n",
    "infersent.build_vocab(sentences, tokenize=True)\n",
    "dict_embeddings = {}\n",
    "for i in range(len(sentences)):\n",
    "print(i)\n",
    "dict_embeddings[sentences[i]] = infersent.encode([sentences[i]], tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83736485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Environmentalists are concerned about the loss of biodiversity that will result from the destruction of the forest, and also about the release of the carbon contained within the vegetation, which could accelerate global warming.\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = infersent.encode(sentences, tokenize=True)\n",
    "Next, Code the train_nli.py in the IDE:\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from data import get_nli, get_batch, build_vocab\n",
    "from mutils import get_optimizer\n",
    "from models import NLINet\n",
    "GLOVE_PATH = \"dataset/GloVe/glove.840B.300d.txt\"\n",
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--nlipath\", type=str, default='dataset/SNLI/', help=\"NLI data path (SNLI or MultiNLI)\")\n",
    "parser.add_argument(\"--outputdir\", type=str, default='savedir/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='model.pickle')\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=20)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=0., help=\"encoder dropout\")\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0., help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=0, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"sgd,lr=0.1\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_nltk_tree(sent.root) pretty_print() for sent in en_nlp(predicted.iloc[ 0, 2]).sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4983fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent is doc.sents:\n",
    "roots = [st.stem(chunk.root. head.text.lower()) for chunk in sent.noun_chunks ]\n",
    "prlnt(roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "print(lemmatizer.mode)  # 'rule'\n",
    "doc = nlp(\"The use of remote sensing for the conservation of the Amazon is also being used by the indigenous tribes of the basin to protect their tribal lands from commercial interests.\")\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd2b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import ast \n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "en_nlp = spacy.load('en')\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "Load the dataset CSV file:\n",
    "\n",
    "data = pd.read_csv(\"train_detect_sent.csv\").reset_index(drop=True)\n",
    "Letâ€™s retrieve the Abstract Syntax Tree for the DataFrames:\n",
    "\n",
    "ast.literal_eval(data[\"sentences\"][0])\n",
    "After all, create a feature for the Data Frames and then train the model:\n",
    "\n",
    "def crt_feature(data):\n",
    "    train = pd.DataFrame()\n",
    "    for k in range(len(data[\"euclidean_dis\"])):\n",
    "        dis = ast.literal_eval(data[\"euclidean_dis\"][k])\n",
    "        for i in range(len(dis)):\n",
    "            train.loc[k, \"column_euc_\"+\"%s\"%i] = dis[i]\n",
    "    print(\"Finished\")\n",
    "    for k in range(len(data[\"cosine_sim\"])):\n",
    "        dis = ast.literal_eval(data[\"cosine_sim\"][k].replace(\"nan\",\"1\"))\n",
    "        for i in range(len(dis)):\n",
    "            train.loc[k, \"column_cos_\"+\"%s\"%i] = dis[i]\n",
    "    train[\"target\"] = data[\"target\"]\n",
    "    return train\n",
    "train = crt_feature(data)\n",
    "train.head(3).transpose()\n",
    "Train the model using Multinomial Logistic regression:\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X,\n",
    "train.iloc[:,-1], train_size=0.8, random_state = 5)\n",
    "mul_lr = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "mul_lr.fit(train_x, train_y)\n",
    "print(\"Multinomial Logistic regression Train Accuracy : \", metrics.accuracy_score(train_y, mul_lr.predict(train_x)))\n",
    "print(\"Multinomial Logistic regression Test Accuracy : \", metrics.accuracy_score(test_y,mul_lr.predict(test_x)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
